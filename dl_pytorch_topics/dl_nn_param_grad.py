import torch
import torch.nn as nn

# Create a learnable parameter
p = nn.Parameter(torch.randn(2, 2))

# Dummy loss to generate gradient
loss = (p ** 2).sum()
loss.backward()

# Print types
print("Type of p:            ", type(p))              # <class 'torch.nn.parameter.Parameter'>
print("Type of p.data:       ", type(p.data))         # <class 'torch.Tensor'>
print("Type of p.grad:       ", type(p.grad))         # <class 'torch.Tensor'>
print("Type of p.grad.data:  ", type(p.grad.data))    # <class 'torch.Tensor'>



print("p.grad:", p.grad)
print("p.grad.data:", p.grad.data)

# Try modifying both
# Both p.grad and p.grad.data share the same storage
p.grad += 1                 # Will be tracked change grad
p.grad.data += 1            # NOT tracked, might silently corrupt gradients still chage grad

print("After modifying:")
print("p.grad:", p.grad)
print("p.grad.data:", p.grad.data)



'''
Great question. In PyTorch, `p.grad` and `p.grad.data` **both give you the gradient tensor**, but they behave **very differently** in terms of autograd and safety.

---

### ‚úÖ 1. `p.grad`

* A regular `torch.Tensor`
* **Tracked** by autograd
* Safe to read or modify (with `torch.no_grad()`)

```python
p.grad += 1  # Unsafe, modifies graph
with torch.no_grad():
    p.grad += 1  # ‚úÖ Safe
```

---

### ‚ö†Ô∏è 2. `p.grad.data`

* A **raw tensor**: `p.grad.detach()` is essentially the same
* **Not tracked** by autograd ‚Äî detached from the graph
* **Unsafe** to write to directly because it bypasses the computational graph
Detach returns a Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks.

```python
p.grad.data += 1  # ‚ö†Ô∏è Modifies the tensor silently, no graph tracking
```

Modifying `.data` bypasses all gradient tracking ‚Äî PyTorch warns against this because it may lead to silent errors in training.

---

### üß™ Minimal Demo

```python
import torch
from torch.nn import Parameter

p = Parameter(torch.randn(2, 2))
loss = (p ** 2).sum()
loss.backward()

print("p.grad:", p.grad)
print("p.grad.data:", p.grad.data)

# Try modifying both
p.grad += 1                 # Will be tracked
p.grad.data += 1            # NOT tracked, might silently corrupt gradients

print("After modifying:")
print("p.grad:", p.grad)
```

---

### ‚úÖ Safer Alternative

If you want to manipulate `grad` without autograd:

```python
with torch.no_grad():
    p.grad += 1
```

This is the preferred way to perform **in-place updates** safely.

---

### üîÅ Summary

| Attribute         | Autograd Tracked? | Safe to Use?         | Use Case                       |
| ----------------- | ----------------- | -------------------- | ------------------------------ |
| `p.grad`          | ‚úÖ Yes             | ‚úÖ Yes (with care)    | Reading or modifying gradients |
| `p.grad.data`     | ‚ùå No              | ‚ö†Ô∏è Risky (avoid)     | Internal hacks (legacy use)    |
| `p.grad.detach()` | ‚ùå No              | ‚úÖ Safer than `.data` | Temporary read/copy            |

Let me know if you want to explore how this relates to hooks or custom optimizers!



'''
